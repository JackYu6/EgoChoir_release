[![Website Badge](https://raw.githubusercontent.com/referit3d/referit3d/eccv/images/project_website_badge.svg)](https://yyvhang.github.io/EgoChoir/)
[![arXiv](https://img.shields.io/badge/arXiv-2405.13659-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2405.13659)
# EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views
PyTorch implementation of **EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views**. The repository will gradually release training, evaluation, inference codes, pre-trained models and the dataset.

## üìñ To Do List
1. - [ ] release the inference, training and evaluation code.
2. - [ ] release the pretrained checkpoint.
3. - [ ] release the collected dataset.

## ‚úâÔ∏è Statement
This project is for research purpose only, please contact us for the licence of commercial use. For any other questions please contact [yyuhang@mail.ustc.edu.cn](yyuhang@mail.ustc.edu.cn).

## üîç Citation

```
@article{yang2024egochoir,
  title={EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views},
  author={Yang, Yuhang and Zhai, Wei and Wang, Chengfeng and Yu, Chengjun and Cao, Yang and Zha, Zheng-Jun},
  journal={arXiv preprint arXiv:2405.13659},
  year={2024}
}
```